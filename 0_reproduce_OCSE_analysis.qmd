---
title: "Results in Uto et al paper"
format:
  html:
    embed-resources: true
    self-contained-math: true
    html-math-method: katex
editor: source
---

Here we reproduce the model in Uto et al. using nimble. 

I will use the notation in the paper. 

Consider $N$ examinees who are independently assessed on multiple $I$ items by multiple $R$ raters. Each examinee is rated by a subset of raters $\mathcal{R}_j$, with cardinality $|\mathcal{R}_j|\leq R$, on the same set of items $i=1,\dots, I$.
We let $\mathcal{J}_r = \{ j: r \in \mathcal{R}_j\}$ be the set of subjects scored by rater $r=1,\dots, R$. Let $Y_{jir}$ for $j=1,\dots,J$, $r \in \mathcal{R}_j$ and $i=1,\dots,I$ be a categorical random variable recording the examinee $j$'s score given by rater $r$ to item $i$. We consider, without loss of generality, that $Y_{jir} \in \{0, 1, \ldots, K\}$, for $i=1.\dots, I$, that is, all items comprise the same range of ordered categorical values. We further assume that the items measure a unidimensional construct modelled by a subject-specific latent trait $\theta_j \in \mathbb{R}$.  

The model in Uto et al has the following form: 

\begin{equation}
\mathbb{P}[Y_{jir}=k|Y_{jir} \in \{k,k-1\},\theta_j, \phi_{ir}, \delta_{irk}]= 
\frac{exp\{D* \phi_{ir}(\theta_j - \delta_{irk})\}}{1+exp\{D* \phi_{ir}(\theta_j - \delta_{irk})\}}  
\end{equation}

for $k=1,\dots, K;$ $j=1,\dots, N;$ $i=1,\dots, I$ and $r \in \mathcal{R}_j$.

The scale term is decomposed as 
\begin{equation}
 \phi_{ir} = \alpha_{i} \alpha_{r}
\end{equation}
where $\alpha_{i}$ represents the discriminatory power of item $i$, $\alpha_{r}$  represents the consistency of rater $r$.

In the location part, the novely is that the model considers a raterâ€“item interaction parameter representing the rater severity for valuation item ($\beta_{ir}$) and an item-specific step-difficulty parameter representing the difference
in rating scales among evaluation items ($d_{rm}$).

\begin{equation}
  \delta_{irk} = \beta_{ir} + d_{im} + d_{rm}
\end{equation}


**Note**: In the paper the term in the exponential is multiplied by $D = 1.7$ which is the scaling constant used to minimize the difference between the normal and logistic distribution functions.

### Constraints

Scale.  $\sum_{r = 1}^R \log \alpha_r = 0$

Location. 

  - $d_{r1} = 0$; $\sum_{k = 2}^K d_{rk} = 0$

  - $d_{i1} = 0$; $\sum_{k = 2}^K d_{ik} = 0$
  

### Results

Using 10,000 mcmc iteration with a burn-in of 4,000. 

```{r}
#| echo: false 
library(ggplot2)
args <- list(resFileName = "output/OSCE_Long/para/para_uto.rds")

listLength <- length(strsplit(args$resFileName, "\\/|.rds")[[1]])
data <- strsplit(args$resFileName, "\\/|.rds")[[1]][listLength -2]
fileName <- strsplit(args$resFileName, "\\/|.rds")[[1]][listLength]

## modelType - parametric or semi
modelType <- strsplit(basename(fileName), "\\_|.rds")[[1]][1]

## read objects
resObj <- readRDS(args$resFileName)
samples <- resObj$samples

```



**Estimates fron nimble**

```{r}
alpha_r <- samples[, grep("^trans_alpha_r", colnames(samples))] |> colMeans()

## apply constraints on the posterior means for the scaling
get_prod_restricted_prm <- function(d){
  return(append(1.0/prod(d), d))
}
alpha_r<- get_prod_restricted_prm(alpha_r[2:5])


#######
d_rk <- samples[, grep("^category_est_r", colnames(samples))] |> colMeans() |> matrix(nrow = 5)

beta_ir <- samples[, grep("^beta_ir", colnames(samples))] |> colMeans() 

severity <- beta_ir |> matrix(nrow = 30, byrow = FALSE) |> colMeans()


tab <- data.frame("alpha_r" = alpha_r, 
                  "d_{r1}" = d_rk[, 1], 
                  "d_{r2}" = d_rk[, 2], 
                  "d_{r3}" = d_rk[, 3], 
                  "d_{r4}" = d_rk[, 4], 
                  "severity" = severity,
                  row.names = NULL)

knitr::kable(tab, col.names = c("$\\alpha_r$", "$d_{r1}$", 
                                "$d_{r2}$", "$d_{r3}$", 
                                "$d_{r4}$", "severity"))
```

![paper_results](uto_et_al/uto_paper_rater_parameters.png)

Note: the difference is in that in the paper they apply the constraint on the posterior means; not sure if this is necessary. 

```{r}
```

**Posterior distribution of the latent trait (parametric model)**

Here I am looking at the distribution of all MCMC samples of the latent trait. 

```{r}

hist(samples[, grep("^eta", colnames(samples))], 
	breaks = 20, prob= TRUE, ylim = c(0,1), 
	main = "Posterior distribution of the latent trait", 
	xlab = "Latent trait")
lines(density(samples[, grep("^eta", colnames(samples))]), col = 4, lwd = 2)


alpha_i <- samples[, grep("^alpha_i", colnames(samples))] |> colMeans()

```

### Semiparametric version

I removed the multiplication by 1.7. Using a DP mixture for the latent ability.  Prior on DP concentration parameter is $Gamma(1,3)$ (expected number of clusters a priori is around 2 with variance 2). Also here it make sense to consider the variance for the cluster components; since there is not huge variability on the scores scale, I would consider a small variance.


```{r}
#| echo: false 

args <- list(resFileName = "output/OSCE_Long/semi/semi_uto.rds")

listLength <- length(strsplit(args$resFileName, "\\/|.rds")[[1]])
data <- strsplit(args$resFileName, "\\/|.rds")[[1]][listLength -2]
fileName <- strsplit(args$resFileName, "\\/|.rds")[[1]][listLength]

## modelType - parametric or semi
modelType <- strsplit(basename(fileName), "\\_|.rds")[[1]][1]

## read objects
resObj <- readRDS(args$resFileName)
samples <- resObj$samples

```



**Estimates fron nimble**

```{r}
alpha_r <- samples[, grep("^trans_alpha_r", colnames(samples))] |> colMeans()

## apply constraints on the posterior means for the scaling
get_prod_restricted_prm <- function(d){
  return(append(1.0/prod(d), d))
}
alpha_r<- get_prod_restricted_prm(alpha_r[2:5])


#######
d_rk <- samples[, grep("^category_est_r", colnames(samples))] |> colMeans() |> matrix(nrow = 5)

beta_ir <- samples[, grep("^beta_ir", colnames(samples))] |> colMeans() 

severity <- beta_ir |> matrix(nrow = 30, byrow = FALSE) |> colMeans()


tab <- data.frame("alpha_r" = alpha_r, 
                  "d_{r1}" = d_rk[, 1], 
                  "d_{r2}" = d_rk[, 2], 
                  "d_{r3}" = d_rk[, 3], 
                  "d_{r4}" = d_rk[, 4], 
                  "severity" = severity,
                  row.names = NULL)

knitr::kable(tab, col.names = c("$\\alpha_r$", "$d_{r1}$", 
                                "$d_{r2}$", "$d_{r3}$", 
                                "$d_{r4}$", "severity"))
```


```{r}
source("functions/estimateDPdensity.R")

hist(samples[, grep("^eta", colnames(samples))], 
	breaks = 25, prob= TRUE, ylim = c(0, 1), 
	main = "Posterior distribution of the latent trait - MCMC samples", 
	xlab = "Latent trait")
lines(density(samples[, grep("^eta", colnames(samples))]), col = 4, lwd = 2)


out <- estimateDPdensity(samples, 
                         paramNames = list(alpha = "alpha_dp", 
                                          muTilde = "muTilde", 
                                          s2Tilde = "s2Tilde", 
                                          zi = "zi"), 
                         nIndividuals = 30, 
	grid = seq(-5, 5, length = 200))

res <- data.frame(grid = out$grid, mean = apply(out$densitySamples, 2, mean))

ggplot(data = res, 
                aes(x = grid, y = mean)) + 
  geom_line()

hist(apply(samples[, grep("^eta", colnames(samples))], 2, mean),
     freq = FALSE, xlim = c(-3,3),
	breaks = 10, prob= TRUE, ylim = c(0,1), 
	main = "Posterior distribution of the latent trait", 
	xlab = "Latent trait")## pointwise estimate of the density for standardized log grid
lines(out$grid, apply(out$densitySamples, 2, mean), lwd = 2, col = 'black') 
lines(out$grid, apply(out$densitySamples, 2, quantile, 0.025), lty = 2, col = 'black') 
lines(out$grid, apply(out$densitySamples, 2, quantile, 0.975), lty = 2, col = 'black') 


hist(samples[, grep("^eta", colnames(samples))],
     freq = FALSE, xlim = c(-3,3),
	breaks = 30, prob= TRUE, ylim = c(0,1), 
	main = "Posterior distribution of the latent trait", 
	xlab = "Latent trait")## pointwise estimate of the density for standardized log grid
lines(out$grid, apply(out$densitySamples, 2, mean), lwd = 2, col = 'black') 
lines(out$grid, apply(out$densitySamples, 2, quantile, 0.025), lty = 2, col = 'black') 
lines(out$grid, apply(out$densitySamples, 2, quantile, 0.975), lty = 2, col = 'black') 

plot(samples[, "alpha_dp"], type = "l")
mean(samples[, "alpha_dp"])

nClusters <- apply(samples[, grep("zi", colnames(samples))], 1, \(x) length(unique(x)))

plot(nClusters, type = "l")
library(salso)

cc <- salso(samples[, grep("zi", colnames(samples))])
cc

densityDPMeasure <- matrix(0, ncol = length(out$grid), nrow = length(resObj$outputG))

for(i in seq_len(length(resObj$outputG))) {  
    densityDPMeasure[i, ] <- sapply(out$grid,
                function(x)(
                  sum(resObj$outputG[[i]][,1]*
                    dnorm(x, resObj$outputG[[i]][,2], sqrt(resObj$outputG[[i]][,3])))))
}

res2 <- data.frame(grid = out$grid, mean = apply(densityDPMeasure, 2, mean))

ggplot(data = res2,aes(x = grid, y = mean)) + 
  geom_line()

```
